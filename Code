Convolutional-Neural-Network
Implementation of Convolutional Neural Model for Image Style Transfer using Deep learning like PyTorch and TensorFlow in Python

###############################################################################################################################

###########################Implementation in PyTorch########################################################################### import pandas as pd import copy

Torch & Tensorflow
import torch 
import torch.nn as nn 
import torch.nn.functional as F 
import torchvision import tensorflow as tf

Visualization
from torchviz import make_dot 
from PIL import Image import matplotlib.pyplot as plt 
%matplotlib inline

import warnings

#####Configuration##### device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

desired size of the output image######
imsize = (512, 512) if torch.cuda.is_available() else (128, 128) # use small size if no gpu

loader = torchvision.transforms.Compose([ torchvision.transforms.Resize(imsize), # scale imported image torchvision.transforms.ToTensor()]) # transform it into a torch tensor

def image_loader(image_name): image = Image.open(image_name)

# fake batch dimension required to fit network's input dimensions
image = loader(image).unsqueeze(0)
return image.to(device, torch.float)

########Plot tensor images as PIL image####
unloader = torchvision.transforms.ToPILImage()

def imshow_tensor(tensor, ax=None): image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension

image = unloader(image)
if ax:
    ax.imshow(image)
else:
    plt.imshow(image)
    
    #####Functions from Part 2 - loss functions#####
class ContentLoss(nn.Module):

def __init__(self, target,):
    super(ContentLoss, self).__init__()
    self.target = target.detach()

def forward(self, input):
    self.loss = F.mse_loss(input, self.target)
    return input
def gram_matrix(input): # Get the size of tensor # a: batch size # b: number of feature maps # c, d: the dimension of a feature map a, b, c, d = input.size()

# Reshape the feature 
features = input.view(a * b, c * d)

# Multiplication
G = torch.mm(features, features.t())  

# Normalize 
G_norm = G.div(a * b * c * d)
return G_norm
class StyleLoss(nn.Module):

def __init__(self, target_feature):
    super(StyleLoss, self).__init__()
    self.target = gram_matrix(target_feature).detach()

def forward(self, input):
    G = gram_matrix(input)
    self.loss = F.mse_loss(G, self.target)
    return input
    
    ######Part 3 - modeling#####
cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device) cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

create a module to normalize input image so we can easily put it in a
nn.Sequential
class Normalization(nn.Module): def init(self, mean, std): super(Normalization, self).init() # .view the mean and std to make them [C x 1 x 1] so that they can # directly work with image Tensor of shape [B x C x H x W]. # B is batch size. C is number of channels. H is height and W is width. self.mean = torch.tensor(mean).view(-1, 1, 1) self.std = torch.tensor(std).view(-1, 1, 1)

def forward(self, img):
    # normalize img
    return (img - self.mean) / self.std
    
    #######Create a sequential model for style transfer#####
desired depth layers to compute style/content losses :
content_layers_default = ['conv_4'] style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

def get_style_model_and_losses(cnn, normalization_mean, normalization_std, style_img, content_img, content_layers=content_layers_default, style_layers=style_layers_default): cnn = copy.deepcopy(cnn)

# normalization module
normalization = Normalization(normalization_mean, normalization_std).to(device)

# just in order to have an iterable access to or list of content/syle
# losses
content_losses = []
style_losses = []

# assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
# to put in modules that are supposed to be activated sequentially
model = nn.Sequential(normalization)

i = 0  # increment every time we see a conv
for n_child, layer in enumerate(cnn.children()):
print()
print(f"n_child: {n_child}")
    if isinstance(layer, nn.Conv2d):
        i += 1
        name = 'conv_{}'.format(i)
    elif isinstance(layer, nn.ReLU):
        name = 'relu_{}'.format(i)
        # The in-place version doesn't play very nicely with the ContentLoss
        # and StyleLoss we insert below. So we replace with out-of-place
        # ones here.
        layer = nn.ReLU(inplace=False)
    elif isinstance(layer, nn.MaxPool2d):
        name = 'pool_{}'.format(i)
    elif isinstance(layer, nn.BatchNorm2d):
        name = 'bn_{}'.format(i)
    else:
        raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

    model.add_module(name, layer)
print(f'Name: {name}')
    if name in content_layers:
print(f'Add content loss {i}')
        # add content loss:
        target = model(content_img).detach()
        content_loss = ContentLoss(target)
        model.add_module("content_loss_{}".format(i), content_loss)
        content_losses.append(content_loss)

    if name in style_layers:
print(f'Add style loss {i}')
        # add style loss:
        target_feature = model(style_img).detach()
        style_loss = StyleLoss(target_feature)
        model.add_module("style_loss_{}".format(i), style_loss)
        style_losses.append(style_loss)

# now we trim off the layers after the last content and style losses
for i in range(len(model) - 1, -1, -1):
    if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
        break

model = model[:(i + 1)]

return model, style_losses, content_losses
###########Load images########### d_path = {} d_path['content'] = tf.keras.utils.get_file('turtle.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Green_Sea_Turtle_grazing_seagrass.jpg') d_path['style'] = tf.keras.utils.get_file('kandinsky.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg') style_img = image_loader(d_path['style'])[:, :, :, :170] content_img = image_loader(d_path['content'])[:, :, :, :170] input_img = content_img.clone()

assert style_img.size() == content_img.size(),
"we need to import style and content images of the same size"

########Modeling###############
Obtain the model for style transfer
with warnings.catch_warnings():
warnings.filterwarnings("ignore") cnn = torchvision.models.vgg19(pretrained=True).features.to(device).eval() model, style_losses, content_losses = get_style_model_and_losses(cnn, cnn_normalization_mean, cnn_normalization_std, style_img, content_img)

######Executing a neural transfer######### ####Gradient Decent######## def get_input_optimizer(input_img): # this line to show that input is a parameter that requires a gradient optimizer = torch.optim.LBFGS([input_img.requires_grad_()]) return optimizer

optimizer = get_input_optimizer(input_img)

Parameters
num_steps = 10 style_weight=5000 content_weight=1 input_img = content_img[:, :, :, :170].clone() d_images = {}

print('Building the style transfer model..') model, style_losses, content_losses = get_style_model_and_losses(cnn, cnn_normalization_mean, cnn_normalization_std, style_img, content_img) optimizer = get_input_optimizer(input_img)

Execution
run = [0] while run[0] <= num_steps:

def closure():
    # correct the values of updated input image
    input_img.data.clamp_(0, 1)

    optimizer.zero_grad()
    model(input_img)
    style_score = 0
    content_score = 0

    for sl in style_losses:
        style_score += sl.loss
    for cl in content_losses:
        content_score += cl.loss

    style_score *= style_weight
    content_score *= content_weight

    loss = style_score + content_score
    loss.backward()

    run[0] += 1
    if run[0] % 2 == 0:
        print("run {}:".format(run))
        print('Style Loss : {:4f} Content Loss: {:4f}'.format(
            style_score.item(), content_score.item()))
        input_img.data.clamp_(0, 1)
        d_images[run[0]] = input_img
        print()

    return style_score + content_score

optimizer.step(closure)

# a last correction...
input_img.data.clamp_(0, 1)

fig, axes = plt.subplots(1, 3, figsize=(20, 8))
d_img = {"Content": content_img, "Style": style_img, "Output": input_img} for i, key in enumerate(d_img.keys()): imshow_tensor(d_img[key], ax=axes[i]) axes[i].set_title(f"{key} Image") axes[i].axis('off')

#####Visualize the process of style transfer#######
fig, axes = plt.subplots(int(len(d_images)/2), 2, figsize=(16, 20)) for i, key in enumerate(d_images.keys()): imshow_tensor(d_images[key], ax=axes[i//2][i%2]) axes[i//2][i%2].set_title("run {}:".format(key)) axes[i//2][i%2].axis('off')

###########################Implementation in TensorFlow########################################################################### import os import tensorflow as tf

Load compressed models from tensorflow_hub
os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'

import IPython.display as display

import matplotlib.pyplot as plt 

import matplotlib as mpl mpl.rcParams['figure.figsize'] = (12,12) mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image import time import functools

def tensor_to_image(tensor): tensor = tensor*255 tensor = np.array(tensor, dtype=np.uint8) if np.ndim(tensor)>3: assert tensor.shape[0] == 1 tensor = tensor[0] return PIL.Image.fromarray(tensor)

######Download images and choose a style image and a content image

content_path = tf.keras.utils.get_file('turtle.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Green_Sea_Turtle_grazing_seagrass.jpg') style_path = tf.keras.utils.get_file('kandinsky.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')

#Visualize the input #Define a function to load an image and limit its maximum dimension to 512 pixels.

def load_img(path_to_img): max_dim = 512 img = tf.io.read_file(path_to_img) img = tf.image.decode_image(img, channels=3) img = tf.image.convert_image_dtype(img, tf.float32)

shape = tf.cast(tf.shape(img)[:-1], tf.float32)
long_dim = max(shape)
scale = max_dim / long_dim

new_shape = tf.cast(shape * scale, tf.int32)

img = tf.image.resize(img, new_shape)
img = img[tf.newaxis, :]
return img


#Create a simple function to display an image:
def imshow(image, title=None): if len(image.shape) > 3: image = tf.squeeze(image, axis=0)

    plt.imshow(image)
    if title:
        plt.title(title)
content_image = load_img(content_path) style_image = load_img(style_path)

plt.subplot(1, 2, 1) imshow(content_image, 'Content Image')

plt.subplot(1, 2, 2) imshow(style_image, 'Style Image')

import tensorflow_hub as hub hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2') stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0] tensor_to_image(stylized_image)

x = tf.keras.applications.vgg19.preprocess_input(content_image*255) x = tf.image.resize(x, (224, 224)) vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet') prediction_probabilities = vgg(x) prediction_probabilities.shape

predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0] [(class_name, prob) for (number, class_name, prob) in predicted_top_5]

vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

print() for layer in vgg.layers: print(layer.name)

content_layers = ['block5_conv2']

style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']

num_content_layers = len(content_layers) num_style_layers = len(style_layers)

def vgg_layers(layer_names):

Creates a vgg model that returns a list of intermediate output values."""
Load our model. Load pretrained VGG, trained on imagenet data
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
vgg.trainable = False

outputs = [vgg.get_layer(name).output for name in layer_names]

model = tf.keras.Model([vgg.input], outputs)
return model

####And to create the model:
style_extractor = vgg_layers(style_layers) style_outputs = style_extractor(style_image*255)

#Look at the statistics of each layer's output for name, output in zip(style_layers, style_outputs): print(name) print(" shape: ", output.numpy().shape) print(" min: ", output.numpy().min()) print(" max: ", output.numpy().max()) print(" mean: ", output.numpy().mean()) print()

def gram_matrix(input_tensor): result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor) input_shape = tf.shape(input_tensor) num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32) return result/(num_locations)

##Extract style and content ####Build a model that returns the style and content tensors.

class StyleContentModel(tf.keras.models.Model): def init(self, style_layers, content_layers): super(StyleContentModel, self).init() self.vgg = vgg_layers(style_layers + content_layers) self.style_layers = style_layers self.content_layers = content_layers self.num_style_layers = len(style_layers) self.vgg.trainable = False

    def call(self, inputs):
    #Expects float input in [0,1]"
        inputs = inputs*255.0
        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
        outputs = self.vgg(preprocessed_input)
        style_outputs, content_outputs = (outputs[:self.num_style_layers], 
                                          outputs[self.num_style_layers:])

        style_outputs = [gram_matrix(style_output)
                         for style_output in style_outputs]

        content_dict = {content_name:value 
                        for content_name, value 
                        in zip(self.content_layers, content_outputs)}

        style_dict = {style_name:value
                      for style_name, value
                      in zip(self.style_layers, style_outputs)}

        return {'content':content_dict, 'style':style_dict}
        
        extractor = StyleContentModel(style_layers, content_layers)
results = extractor(tf.constant(content_image))

print('Styles:') for name, output in sorted(results['style'].items()): print(" ", name) print(" shape: ", output.numpy().shape) print(" min: ", output.numpy().min()) print(" max: ", output.numpy().max()) print(" mean: ", output.numpy().mean()) print()

print("Contents:") for name, output in sorted(results['content'].items()): print(" ", name) print(" shape: ", output.numpy().shape) print(" min: ", output.numpy().min()) print(" max: ", output.numpy().max()) print(" mean: ", output.numpy().mean())

    ######Run gradient descent######
style_targets = extractor(style_image)['style'] content_targets = extractor(content_image)['content'] image = tf.Variable(content_image)

def clip_0_1(image): return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

style_weight=1e-2 content_weight=1e4

def style_content_loss(outputs): style_outputs = outputs['style'] content_outputs = outputs['content'] style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()]) style_loss *= style_weight / num_style_layers

content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) 
                         for name in content_outputs.keys()])
content_loss *= content_weight / num_content_layers
loss = style_loss + content_loss
return loss

@tf.function()
def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs) grad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image))

train_step(image)
train_step(image) train_step(image) tensor_to_image(image)

import time start = time.time()

epochs = 10 steps_per_epoch = 100

step = 0 for n in range(epochs): for m in range(steps_per_epoch): step += 1 train_step(image) print(".", end='') display.clear_output(wait=True) display.display(tensor_to_image(image)) print("Train step: {}".format(step))

end = time.time() print("Total time: {:.1f}".format(end-start))

def high_pass_x_y(image): x_var = image[:,:,1:,:] - image[:,:,:-1,:] y_var = image[:,1:,:,:] - image[:,:-1,:,:] return x_var, y_var x_deltas, y_deltas = high_pass_x_y(content_image) plt.figure(figsize=(14,10)) plt.subplot(2,2,1) imshow(clip_0_1(2y_deltas+0.5), "Horizontal Deltas: Original") plt.subplot(2,2,2) imshow(clip_0_1(2x_deltas+0.5), "Vertical Deltas: Original") x_deltas, y_deltas = high_pass_x_y(image) plt.subplot(2,2,3) imshow(clip_0_1(2y_deltas+0.5), "Horizontal Deltas: Styled") plt.subplot(2,2,4) imshow(clip_0_1(2x_deltas+0.5), "Vertical Deltas: Styled")

plt.figure(figsize=(14,10))

sobel = tf.image.sobel_edges(content_image) plt.subplot(1,2,1) imshow(clip_0_1(sobel[...,0]/4+0.5), "Horizontal Sobel-edges") plt.subplot(1,2,2) imshow(clip_0_1(sobel[...,1]/4+0.5), "Vertical Sobel-edges")

def total_variation_loss(image): x_deltas, y_deltas = high_pass_x_y(image) return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas)) total_variation_loss(image).numpy()

tf.image.total_variation(image).numpy()

total_variation_weight=30 @tf.function() def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs) loss += total_variation_weight*tf.image.total_variation(image)

grad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image))

image = tf.Variable(content_image)

import time start = time.time()

epochs = 10 steps_per_epoch = 100

step = 0 for n in range(epochs): for m in range(steps_per_epoch): step += 1 train_step(image) print(".", end='') display.clear_output(wait=True) display.display(tensor_to_image(image)) print("Train step: {}".format(step))

end = time.time() print("Total time: {:.1f}".format(end-start))
